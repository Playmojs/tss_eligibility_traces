\documentclass{article}
\usepackage{url}
\usepackage{cite}

\title{My masters}
\author{Simen Storesund}
\date{\today}


\begin{document}
    \maketitle
    \section{Introduction}
    \textbf{Introduction} One of the most interesting developments in neuroscience has been the emergence of single cells that seem to represent abstract or complex concepts in their activity patterns. Some of the most groundbreaking findings in this context are cells that have a spatially modulated pattern of activity in the hippocampus and surrounding regions. First, the place cells were found in the mouse hippocampus (citation) in the 1970s. The place cell is remarkable because it fires preferentially when the mouse was in some location in the environment, and was silent otherwise. The research on place cells has been extensive, and the cells have been found both in the CA1 and CA3 region of the hippocampus, and their firing properties and activity patterns have been mapped extensively.
    This made a significant impact in neuroscience because the hippocampus was observed to play an important role in the formation of new episodic memories, and the place cell implied a link between this type of memory and spatial awareness.
    Other findings also show that place cells might play a role in navigation: during sleep, sequential activation of place cells has been observed, a phenomenon coined â€˜replay'. These sequences were both reminiscent of the sequences the mouse had experienced during the day, but it also showed combining place cell sequences into novel paths the mouse had never taken before.
    In addition, during movement, it seems like place cells are fired in sequences that involve places the mouse was recently at, as well as predicting which place-cells will be active in the immediate future. Interestingly, this activity occurs relative to oscillatory activity in the hippocampus, the theta cycles, so the present place cell is active in the trough of the theta wave, the previous place cells were active in the descending part of the wave and the future place cells active in the ascending part. This phenomenon, 'preplay', as well as reply, implies that place cells might play a role in navigation and planning trajectories.
    Since the hippocampus lies downstream of multiple sensory modalities, it has been a working theory that the place fields are formed as a result of combining information from multiple sensory systems. There are multiple models that try to explain how the place fields might form, and one is the boundary vector cell model, which predicts a cell type called the boundary vector cell. This cell was to fire preferentially when there was an environment border in some direction at some distance. The boundary vector cell model theorized that place cells could arise from boundary vector cell inputs alone, as well as direct competition between place cells mediated by lateral inhibition. About ten years later, these boundary vector cells were found in the entorhinal cortex(?) by (mosers?)
    Multiple types of cells with spatial firing patterns have been found in neighboring regions to the hippocampus. The grid cell, first found in 2005 by Haftings et al, is periodically active in a hexagonal pattern in an environment. As opposed to the place cell, which is typically active in only one or two locations in an environment, these grid cells then tile the entire environment.
    Grid cells have been found in layer II, III and V of the lateral entorhinal cortex (as well as other places?), and have interesting firing properties. For one, in a square environment, the hexagonal pattern seems to be offset relative to the wall by about 7.5 degrees. In addition, the spacing between the firing areas differ between grid cells, but in which the different scales differ by factors of the square root of two. Additionally, although the grid cell fires periodically in many environments, the shape of the environment determines whether their spatial activity pattern is truly hexagonal or not.
    (Describe different defects here?)
    Since the first description of the grid cell, plenty of models have tried to account for what purpose it might have, and how the grid pattern emerges. Upon seeing that the grid cell received multiple inputs from velocity cells, which activate preferentially when the animal has some speed in some direction, it was hypothesized that the grid cell might perform path integration. This would allow animals to guess their position even without external sensory cues. The most popular models were either continuous attractor networks (CANs) or oscillatory interference networks, which both can explain the grid patterns.
    However, path integration is inherently prone to drifting, so in recent years other grid cell functions have been popular, focusing more on the grid cells' function in navigation (is this true?)
    However, most models don't try to account seriously for the grid cell activity. According to Marr's three levels of analysis, we would expect that if the grid cell is part of some network with a particular purpose, the grid cell would play an optimal part in that network. As such, a good grid cell model should account for not only what purpose the grid cell has and how the grid pattern is formed, but also why the grid pattern is desired for the grid cell purpose in the first place.
    
    The transitional scale space (TSS) model is a model that was developed with this in mind. The TSS model works under the assumption that place cells form a cognitive map, storing information about previously visited locations in the brain. Under this assumption, navigation would also require a way to connect place cells together to form sequences. The TSS model suggests that a separate network does this, coding for spatial transitions. This transition network consists of transition cells, which by Marr's three levels of analysis should be as efficient as possible. Efficiency, in this case, means that each transition cell should account for as many transitions as possible, without being ambiguous.
    The one factor that must be avoided is having a transition cell that encodes a transition both from and to the same place cell. This is achieved by having the transition cell transition from a series of place cells in some area, and connecting these to place cells in the surrounding area, in a center-surround fashion. By bundling as many of these center areas together as possible, the transition cell is maximally efficient.
    In square or circular environments, this would lead to transition cells having hexagonal firing fields, such as grid cells.
    This model satisfies all levels of Marr's: it explains that grid cells operate in a network that encodes spatial transitions to enable navigation across the place cell network. The hexagonal firing field is optimal for each cell to encode as many transitions as possible, and this firing field can be learnt by packing spatial center-surround receptive fields across the entire environment.
    Waniek showed that only three grid cells would be sufficient to cover the entire environment under ideal conditions. In addition, he showed that packing these receptive fields would lead to a wall angle offset as described in the literature. Moreover, to accelerate navigation, having grid cells with different spacings, in intervals of the square root of two, would also be ideal. 
    Creating cells with grid-like activity according to the TSS-model has been demonstrated in discrete time simulations, in which grid cells associated or dissociated to randomly weighted spatial inputs, and inhibited each other to create winner-takes-all dynamics. To encourage as many center-surround areas as possible, weights were gradually increased to some maximum value. The nature of the spatial input was place-responding cells spaced in an even, rectangular grid across the environment, and it did not produce good results with more than three grid cells.
    An assumption was that the input in reality would be some combination of different existing spatial information, such as head-direction cells, boundary vector cells or object vector cells, but that each of this information would synapse in combination on different grid cell dendrites. Each dendrite would respond maximally to one location in space, which is what the regularly distributed spatial input represented.
    With the goal of mimicking grid cells with methods as plausible as possible, this thesis works to improve the simulation results in two ways. One, the goal is to test if the model can provide grid cells with an actual dendritic layer, using boundary vector cells as the input layer instead of regularly spaced place-like cells. In addition, the thesis seeks to get networks with more than three grid cells by delaying inhibition, so grid cells can have overlapping areas of activity.
    To simulate this, the thesis uses spiking neural networks simulated using the Brian2 library in python. 
    
    In these simulations, spatial inputs were given to the grid cells each theta cycle, and they came in the form of spatial cells that were distributed across the environment in a rectangular grid. The inputs arrived with a delay relative to theta based on their spatial relevance, and synapsed on grid cells with randomly initiated weights. The learning rule involved the winning grid cell associating to the local inputs, and dissociating from more distal inputs. In addition, a baseline factor pushed the weights up to encourage grid cells to be active in as many positions as possible.
    In addition to the unrealistic spatial input, these simulations worked in discrete time with instantaneous inhibition, and failed to produce grid cells when the grid cell network contained more than three neurons.
    To lift these implausible assumptions, the goal of this work is to implement this network in a spiking neural network simulated in approximately continuous time. The goal is to enable grid cell networks with more than three grid cells, by allowing overlapping grid fields due to delayed inhibition, so multiple grid cells can associate to the same location. In addition, this work evaluates what happens when spatially modulated input is replaced by boundary vector cells, similar to the boundary vector cell model.
    
    \section{Methods} 
    \subsection{Overall model design and -goals}The purpose of this thesis is to test gridness in transition neurons as described by the TSS-model with biologically plausible spatial inputs and conditions (continuous time). To achieve this, grid cells were simulated in a spiking neural network, which are artificial networks with certain properties:
    \begin{enumerate}
        \item The network is simulated with continuous time, or in time steps that are vastly shorter than the mean firing rate of neurons.
        \item Neurons communicate in temporally discrete spikes, triggered when an internal voltage variable exceeds a threshold.
    \end{enumerate}
    
    Different network structures have been tested over the course of the thesis, and the results of different structures are treated in the results section. This section will first describe an ideal network, and describe the different assumptions that network would operate under. Further sections will describe other network structures that were used to simplify and test different levels of plausibility.

    An ideal network has the following properties: it is simulated in a biologically plausible network, such as a spiking neural network. Most importantly, this means that communication happens at a delay, time is continuous and learning rules must be online and local. Simulations get inputs from some plausible spatially tuned neurons, more precisely boundary vector cells (BVCs), whose activity is based on real or simulated trajectory data. From these inputs, another layer of cells of arbitrary size learns center- and surrounding area information to represent transition-cells as described in the TSS-model, encoding transitions on a single scale. Under these constraints, the objective is whether the transitional neurons self-organize into single-cells with hexagonal activity patterns in space, and in which the population covers all possible environment transitions.

    Both an ideal network and subsequent simplifications will have some common features to accommodate these limitations: first, spike timing dependent plasticity is a biologically plausible learning rule which both allows single transition cells to associate with spatial inputs associated with center, and dissociate with inputs associated with surround areas. To facilitate this, all inputs arrive in a theta-wave-like pulse, at 10 Hz, in which only spatial input with sufficiant relevance is active, and the input is phase coded so the less relevant, the higher the temporal offset is relative to theta. This means that a firing transition cell is likely to fire following center-inputs, and subsequently receive a burst of surrounding inputs. The STDP learning-rule will ensure that the center-activity which arrived pre-spike are potentiated, and the surround-activity arriving post-spike is depressed. Secondarily, all active inputs are slightly potentiated to encourage the transition cell to be active in as many center-areas across the environment as possible.
    
    An inhibitory layer connects transition cells laterally, causing strong inhibition, but on a delay. This leaves a narrow window in which multiple grid cells can fire, followed by complete inhibition, so some grid cells partially associate to the same areas.

    Due to the high number of parameters available in such a network, the network would preferably see characteristics of the transition cells, such as grid-patterns, emerge quickly and without perfectly honing parameters.
    This is both because a network structure that produces grid cells only with very particular parameters is less biologically plausible, and because testing the parameter space properly is very time consuming. On this note, there are reasons to think that a network in which grid cell activity occurs in transition cells based on linear summation of BVC input, as shown in figure 2.1, is difficult to achieve. For one, boundary vector inputs in neighboring areas are typically highly linearly correlated, so the grid cell would struggle to associate to one location and dissociate from another. Secondly, for instance in square environments, one could imagine picking two locations that the transition cell has associated with, denoted \((x_1, y_1)\) and \((x_2, y_2)\). In locations \((x_1, y_2)\) and \((x_2, y_1)\), the input would be highly similar to half the input from position 1 and half the input from position 2, but the transition cell should not fire in these locations, because that would lead to rectangular-grid firing patterns, not hexagonal. It should be noted here that the spiking neural network itself implies non-linear features that might have enabled such a network to produce grid-cells. One way to achieve this would be with other transition cells that are sufficiently active in location \((x_1, y_2)\) and \((x_2, y_1)\), so they inhibit the first transition cell. However, due to the fine-tuning such a network would require, this network structure was not directly pursued in this work.

    An appealing alternative, which highlights the advantage of biological neurons, was to consider nonlinear dendritic computation on transition cells in a multi compartment model. Under this idea, although boundary vector cells technically synapse onto grid cells directly, the simulation treats dendrites as separate units within one transition cell. Each dendrite would get inputs from some subset of BVCs, and the transition cell can associate and dissociate to all inputs on an entire dendrite. Similarly to the BVC-model, each dendrite would preferably be active in only single locations in the environment, for instance by responding non-linearly to inputs with a soft-max activation function (as in the other paper). In the example above, this would let the transition cell associate to dendrites responding highly to \((x_1, y_1)\) or \((x_2, y_2)\), while remaining dissociated from dendrites activated in \((x_1, y_2)\) or \((x_2, y_1)\), without sacrificing biological plausibility. This multi compartment model satisfies all requirements stated above. 

    However, to shorten simulation time and reduce complexity, this kind of network can be simplified. This can be done incrementally, in which each step simplifies complexity but also reduces plausibility:
    \begin{enumerate}
        \item The BVC-model showed that place-like activity can be produced from BVC-inputs. As such, BVC-input can be replaced by place-like inputs directly as non-spiking dendrites in the multi-compartment model. 
        \item To reduce the number of neurons, each dendrite can connect to each grid-cell, and the dendrites can be spiking to reduce computational time for each of them. Here, the dendrites still fire in random places, according to some distribution.
        \item The dendrites can respond to places distributed in a regular, rectangular grid.
    \end{enumerate}
 
    Note that step 3 is highly similar to the already existing simulations by Waniek (citation?), just in a simulation with spiking neurons. This was useful, because it allowed testing the hypothesis that grid cells can exist in networks of more than three if the inhibition is delayed, not showed in previous simulations. Then, networks could gradually be made more complex and biologically plausible, leading to a series of networks that could be tested sequentially from simple and less plausible to more complex and more plausible. The concrete implementation of each network and other considerations are treated in their own upcoming subsections.

    \subsection{Rectangularly spaced inputs} This network structure is heavily inspired by previous TSS-simulations, with three layers: input neurons, grid cells and inhibitory neurons. The input neurons have a preferred spatial location, and have the chance of being activated each theta cycle, which is set to happen with a 10 Hz frequency. The activation function for input neuron i with position \((x_i, y_i)\) is \[delay = \frac{\sqrt{(x_i - x)^2 + (y_i - y)^2}}{\sigma} + \mu\]
    in which x and y is the current location, \(\sigma\) is a scale parameter determining the width of input, and \(\mu\) is noise. Typically, the method used a cutoff at 20 ms, so only reasonably active inputs would activate, but this cutoff was arbitrary. Each of these inputs synapsed on each grid cell, with weights uniformly distributed between 0 and some maximum, \(w_{max}\). The grid cell had a voltage parameter which triggered spikes if it surpassed a treshhold, or updated according to the formula: \[ Insert formula here: I don't know how to get this into a single thing...\].
    The STDP learning rule was implemented by separating potentiation and depression: upon grid cell firing, potentiation for weight i worked according to the formula \[ w_i = clip(w_i + a^{pre}_i*\nu, 0, w_{max})\] in which \(a^{pre}_i\) is a variable incremented when input i fires, and decays with time. \(\nu\) is here a learning rate, which can be constant or variable. 
    Similarly, upon presynaptic input from input i, weights are depressed, but also increased according to a baseline parameter, according to \[ w_i = clip(w_i + (a^{post}_i + \alpha * (w_{max}-w_i))*\nu, 0, w_{max})\] where \(a^{post}_i\) is a symmetric parameter to \(a^{pre}_i\), but which decrements upon post synaptic firing and decays at a separate rate. The baseline weight increase operates with rate \(\alpha\).
    Grid cells then interact by activating inhibitory neurons which inhibit all grid cells for a time, typically by reducing their voltage by a large, constant amount, but which is small enough so the voltage returns to zero by the next theta.
    In this model, all synapses were given a delay, but the feedforward nature of inputs to grid cells made the delay here redundant.

    \subsection{Randomly spaced inputs}
    This network structure had similar structure and learning rule to the network in the section above. In these networks, however, there is some randomness in determining which area an input neuron responded to. This was implemented in three categories: one, the inputs were first arranged in a grid, and then randomly jittered slightly in some random direction. Two, the inputs were distributed in a blue noise like pattern, to ensure a relatively even, although not regular, distribution. Three, the inputs were distributed in a white noise like pattern, in which one input neuron would respond to areas independently of other areas.
    While inputs responding to areas in a uniformly random, independent manner would be the easiest, from the perspective of BVC-to-grid cell inputs, it brings the disadvantage that some areas will be less covered, so a grid cell that's supposed to fire in some region according to its grid might simply not receive any input there at all.
    Inputs that were regular, but jittered, were generated by first generating regularly spaced inputs, and adding some normally distributed noise with variance \(\sigma_j\), \[noise ~ N(0, \sigma_j)\]
    The blue noise inputs was generated by iteratively generating a number of uniformly distributed suggested points, adding the point that was the furthest from all existing points to the existing points, and repeating until the desired number of points was reached. In my simulations, the number of suggested points was directly proportional the number of existing points.
    White noise was made by generating the desired number of uniformly distributed points independently of each other.

    \subsection{Gap junction inputs}
    To further bridge the gap between the place-like inputs described thus far and BVCS, this network structure had an intermediary layer between inputs and grid cells. This layer was supposed to resemble dendrites, under the constraint that each dendrite activates preferrably in one location. This way, each dendrite only received input from a single input neuron, and only synapsed on a single grid cell. The nature of this synapse only synapsed in the simulation of the network, of course, which was achieved by making grid cells non-spiking, and the synapse itself resembled a gap-junction.
    As such, while the dendrite's voltage would behave as expected, increasing when receiving inputs and decaying towards zero over time, the grid cell's voltage was determined by the following formula: \[ v_{grid cell} = \sum_{i}^{} c_i * tanh(v_i)\] Here, the nonlinear function \(tanh(v_i)\) gives the dendrite a softmax-like behavior, so the dendrite at anytime is either activated or not. \(c_i\) is the dendrite conductance, reflecting how effectively the dendrite's voltage affects the grid cell voltage. The learning rules described in 2.2 is here moved to affect the conductance instead of synapse weights, but is otherwise similar.
    This network structure was used for both regular and randomized inputs.

    \subsection{Boundary vector cell inputs}
    This network structure was designed to be as biologically plausible as possible, but it builds directly on the past sections. As in the gap junction networks, this one has \textit{maybe actually commence this one before continuing, lol}


\bibliography{Text.bib}{}
\bibliographystyle{plain}
\end{document}